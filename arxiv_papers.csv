total_results,papers
15,"[{'id': '2601.19884v1', 'title': 'SONIC: Spectral Oriented Neural Invariant Convolutions', 'authors': ['Gijs Joppe Moens', 'Regina Beets-Tan', 'Eduardo H. P. Pooch'], 'abstract': 'Convolutional Neural Networks (CNNs) rely on fixed-size kernels scanning local patches, which limits their ability to capture global context or long-range dependencies without very deep architectures. Vision Transformers (ViTs), in turn, provide global connectivity but lack spatial inductive bias, depend on explicit positional encodings, and remain tied to the initial patch size. Bridging these limitations requires a representation that is both structured and global. We introduce SONIC (Spectral Oriented Neural Invariant Convolutions), a continuous spectral parameterisation that models convolutional operators using a small set of shared, orientation-selective components. These components define smooth responses across the full frequency domain, yielding global receptive fields and filters that adapt naturally across resolutions. Across synthetic benchmarks, large-scale image classification, and 3D medical datasets, SONIC shows improved robustness to geometric transformations, noise, and resolution shifts, and matches or exceeds convolutional, attention-based, and prior spectral architectures with an order of magnitude fewer parameters. These results demonstrate that continuous, orientation-aware spectral parameterisations provide a principled and scalable alternative to conventional spatial and spectral operators.', 'categories': ['cs.CV', 'cs.LG'], 'published': '2026-01-27T18:51:11+00:00', 'url': 'https://arxiv.org/pdf/2601.19884v1', 'resource_uri': 'arxiv://2601.19884v1'}, {'id': '2601.18392v1', 'title': 'Efficient Complex-Valued Vision Transformers for MRI Classification Directly from k-Space', 'authors': ['Moritz Rempe', 'Lukas T. Rotkopf', 'Marco Schlimbach', 'Helmut Becker', 'Fabian Hörst', 'Johannes Haubold', 'Philipp Dammann', 'Kevin Kröninger', 'Jens Kleesiek'], 'abstract': 'Deep learning applications in Magnetic Resonance Imaging (MRI) predominantly operate on reconstructed magnitude images, a process that discards phase information and requires computationally expensive transforms. Standard neural network architectures rely on local operations (convolutions or grid-patches) that are ill-suited for the global, non-local nature of raw frequency-domain (k-Space) data. In this work, we propose a novel complex-valued Vision Transformer (kViT) designed to perform classification directly on k-Space data. To bridge the geometric disconnect between current architectures and MRI physics, we introduce a radial k-Space patching strategy that respects the spectral energy distribution of the frequency-domain. Extensive experiments on the fastMRI and in-house datasets demonstrate that our approach achieves classification performance competitive with state-of-the-art image-domain baselines (ResNet, EfficientNet, ViT). Crucially, kViT exhibits superior robustness to high acceleration factors and offers a paradigm shift in computational efficiency, reducing VRAM consumption during training by up to 68$\\times$ compared to standard methods. This establishes a pathway for resource-efficient, direct-from-scanner AI analysis.', 'categories': ['cs.CV'], 'published': '2026-01-26T11:50:52+00:00', 'url': 'https://arxiv.org/pdf/2601.18392v1', 'resource_uri': 'arxiv://2601.18392v1'}, {'id': '2601.17987v1', 'title': 'Systematic Characterization of Minimal Deep Learning Architectures: A Unified Analysis of Convergence, Pruning, and Quantization', 'authors': ['Ziwei Zheng', 'Huizhi Liang', 'Vaclav Snasel', 'Vito Latora', 'Panos Pardalos', 'Giuseppe Nicosia', 'Varun Ojha'], 'abstract': 'Deep learning networks excel at classification, yet identifying minimal architectures that reliably solve a task remains challenging. We present a computational methodology for systematically exploring and analyzing the relationships among convergence, pruning, and quantization. The workflow first performs a structured design sweep across a large set of architectures, then evaluates convergence behavior, pruning sensitivity, and quantization robustness on representative models. Focusing on well-known image classification of increasing complexity, and across Deep Neural Networks, Convolutional Neural Networks, and Vision Transformers, our initial results show that, despite architectural diversity, performance is largely invariant and learning dynamics consistently exhibit three regimes: unstable, learning, and overfitting. We further characterize the minimal learnable parameters required for stable learning, uncover distinct convergence and pruning phases, and quantify the effect of reduced numeric precision on trainable parameters. Aligning with intuition, the results confirm that deeper architectures are more resilient to pruning than shallower ones, with parameter redundancy as high as 60%, and quantization impacts models with fewer learnable parameters more severely and has a larger effect on harder image datasets. These findings provide actionable guidance for selecting compact, stable models under pruning and low-precision constraints in image classification.', 'categories': ['cs.LG', 'cs.CV'], 'published': '2026-01-25T20:31:10+00:00', 'url': 'https://arxiv.org/pdf/2601.17987v1', 'resource_uri': 'arxiv://2601.17987v1'}, {'id': '2601.17352v1', 'title': 'HyDeMiC: A Deep Learning-based Mineral Classifier using Hyperspectral Data', 'authors': ['M. L. Mamud', 'Piyoosh Jaysaval', 'Frederick D Day-Lewis', 'M. K. Mudunuru'], 'abstract': ""Hyperspectral imaging (HSI) has emerged as a powerful remote sensing tool for mineral exploration, capitalizing on unique spectral signatures of minerals. However, traditional classification methods such as discriminant analysis, logistic regression, and support vector machines often struggle with environmental noise in data, sensor limitations, and the computational complexity of analyzing high-dimensional HSI data. This study presents HyDeMiC (Hyperspectral Deep Learning-based Mineral Classifier), a convolutional neural network (CNN) model designed for robust mineral classification under noisy data. To train HyDeMiC, laboratory-measured hyperspectral data for 115 minerals spanning various mineral groups were used from the United States Geological Survey (USGS) library. The training dataset was generated by convolving reference mineral spectra with an HSI sensor response function. These datasets contained three copper-bearing minerals, Cuprite, Malachite, and Chalcopyrite, used as case studies for performance demonstration. The trained CNN model was evaluated on several synthetic 2D hyperspectral datasets with noise levels of 1%, 2%, 5%, and 10%. Our noisy data analysis aims to replicate realistic field conditions. The HyDeMiC's performance was assessed using the Matthews Correlation Coefficient (MCC), providing a comprehensive measure across different noise regimes. Results demonstrate that HyDeMiC achieved near-perfect classification accuracy (MCC = 1.00) on clean and low-noise datasets and maintained strong performance under moderate noise conditions. These findings emphasize HyDeMiC's robustness in the presence of moderate noise, highlighting its potential for real-world applications in hyperspectral imaging, where noise is often a significant challenge."", 'categories': ['cs.CV'], 'published': '2026-01-24T07:57:01+00:00', 'url': 'https://arxiv.org/pdf/2601.17352v1', 'resource_uri': 'arxiv://2601.17352v1'}, {'id': '2601.17180v1', 'title': 'Conservative & Aggressive NaNs Accelerate U-Nets for Neuroimaging', 'authors': ['Inés Gonzalez-Pepe', 'Vinuyan Sivakolunthu', 'Jacob Fortin', 'Yohan Chatelain', 'Tristan Glatard'], 'abstract': 'Deep learning models for neuroimaging increasingly rely on large architectures, making efficiency a persistent concern despite advances in hardware. Through an analysis of numerical uncertainty of convolutional neural networks (CNNs), we observe that many operations are applied to values dominated by numerical noise and have negligible influence on model outputs. In some models, up to two-thirds of convolution operations appear redundant. We introduce Conservative & Aggressive NaNs, two novel variants of max pooling and unpooling that identify numerically unstable voxels and replace them with NaNs, allowing subsequent layers to skip computations on irrelevant data. Both methods are implemented within PyTorch and require no architectural changes. We evaluate these approaches on four CNN models spanning neuroimaging and image classification tasks. For inputs containing at least 50% NaNs, we observe consistent runtime improvements; for data with more than two-thirds NaNs )common in several neuroimaging settings) we achieve an average inference speedup of 1.67x. Conservative NaNs reduces convolution operations by an average of 30% across models and datasets, with no measurable performance degradation, and can skip up to 64.64% of convolutions in specific layers. Aggressive NaNs can skip up to 69.30% of convolutions but may occasionally affect performance. Overall, these methods demonstrate that numerical uncertainty can be exploited to reduce redundant computation and improve inference efficiency in CNNs.', 'categories': ['cs.LG', 'cs.AI'], 'published': '2026-01-23T21:13:25+00:00', 'url': 'https://arxiv.org/pdf/2601.17180v1', 'resource_uri': 'arxiv://2601.17180v1'}, {'id': '2601.15865v1', 'title': 'A Lightweight Brain-Inspired Machine Learning Framework for Coronary Angiography: Hybrid Neural Representation and Robust Learning Strategies', 'authors': ['Jingsong Xia', 'Siqi Wang'], 'abstract': 'Background: Coronary angiography (CAG) is a cornerstone imaging modality for assessing coronary artery disease and guiding interventional treatment decisions. However, in real-world clinical settings, angiographic images are often characterized by complex lesion morphology, severe class imbalance, label uncertainty, and limited computational resources, posing substantial challenges to conventional deep learning approaches in terms of robustness and generalization.Methods: The proposed framework is built upon a pretrained convolutional neural network to construct a lightweight hybrid neural representation. A selective neural plasticity training strategy is introduced to enable efficient parameter adaptation. Furthermore, a brain-inspired attention-modulated loss function, combining Focal Loss with label smoothing, is employed to enhance sensitivity to hard samples and uncertain annotations. Class-imbalance-aware sampling and cosine annealing with warm restarts are adopted to mimic rhythmic regulation and attention allocation mechanisms observed in biological neural systems.Results: Experimental results demonstrate that the proposed lightweight brain-inspired model achieves strong and stable performance in binary coronary angiography classification, yielding competitive accuracy, recall, F1-score, and AUC metrics while maintaining high computational efficiency.Conclusion: This study validates the effectiveness of brain-inspired learning mechanisms in lightweight medical image analysis and provides a biologically plausible and deployable solution for intelligent clinical decision support under limited computational resources.', 'categories': ['cs.CV', 'cs.LG'], 'published': '2026-01-22T11:14:37+00:00', 'url': 'https://arxiv.org/pdf/2601.15865v1', 'resource_uri': 'arxiv://2601.15865v1'}, {'id': '2601.15539v1', 'title': 'A Machine Vision Approach to Preliminary Skin Lesion Assessments', 'authors': ['Ali Khreis', ""Ro'Yah Radaideh"", 'Quinn McGill'], 'abstract': 'Early detection of malignant skin lesions is critical for improving patient outcomes in aggressive, metastatic skin cancers. This study evaluates a comprehensive system for preliminary skin lesion assessment that combines the clinically established ABCD rule of dermoscopy (analyzing Asymmetry, Borders, Color, and Dermoscopic Structures) with machine learning classification. Using a 1,000-image subset of the HAM10000 dataset, the system implements an automated, rule-based pipeline to compute a Total Dermoscopy Score (TDS) for each lesion. This handcrafted approach is compared against various machine learning solutions, including traditional classifiers (Logistic Regression, Random Forest, and SVM) and deep learning models. While the rule-based system provides high clinical interpretability, results indicate a performance bottleneck when reducing complex morphology to five numerical features. Experimental findings show that transfer learning with EfficientNet-B0 failed significantly due to domain shift between natural and medical images. In contrast, a custom three-layer Convolutional Neural Network (CNN) trained from scratch achieved 78.5% accuracy and 86.5% recall on median-filtered images, representing a 19-point accuracy improvement over traditional methods. The results demonstrate that direct pixel-level learning captures diagnostic patterns beyond handcrafted features and that purpose-built lightweight architectures can outperform large pretrained models for small, domain-specific medical datasets.', 'categories': ['eess.IV', 'cs.AI', 'cs.CV', 'cs.LG'], 'published': '2026-01-21T23:48:59+00:00', 'url': 'https://arxiv.org/pdf/2601.15539v1', 'resource_uri': 'arxiv://2601.15539v1'}, {'id': '2601.14678v1', 'title': 'Transfer Learning from One Cancer to Another via Deep Learning Domain Adaptation', 'authors': ['Justin Cheung', 'Samuel Savine', 'Calvin Nguyen', 'Lin Lu', 'Alhassan S. Yasin'], 'abstract': 'Supervised deep learning models often achieve excellent performance within their training distribution but struggle to generalize beyond it. In cancer histopathology, for example, a convolutional neural network (CNN) may classify cancer severity accurately for cancer types represented in its training data, yet fail on related but unseen types. Although adenocarcinomas from different organs share morphological features that might support limited cross-domain generalization, addressing domain shift directly is necessary for robust performance. Domain adaptation offers a way to transfer knowledge from labeled data in one cancer type to unlabeled data in another, helping mitigate the scarcity of annotated medical images.\n  This work evaluates cross-domain classification performance among lung, colon, breast, and kidney adenocarcinomas. A ResNet50 trained on any single adenocarcinoma achieves over 98% accuracy on its own domain but shows minimal generalization to others. Ensembling multiple supervised models does not resolve this limitation. In contrast, converting the ResNet50 into a domain adversarial neural network (DANN) substantially improves performance on unlabeled target domains. A DANN trained on labeled breast and colon data and adapted to unlabeled lung data reaches 95.56% accuracy.\n  We also examine the impact of stain normalization on domain adaptation. Its effects vary by target domain: for lung, accuracy drops from 95.56% to 66.60%, while for breast and colon targets, stain normalization boosts accuracy from 49.22% to 81.29% and from 78.48% to 83.36%, respectively. Finally, using Integrated Gradients reveals that DANNs consistently attribute importance to biologically meaningful regions such as densely packed nuclei, indicating that the model learns clinically relevant features and can apply them to unlabeled cancer types.', 'categories': ['cs.CV', 'cs.AI', 'cs.LG', 'cs.NE', 'q-bio.TO'], 'published': '2026-01-21T05:50:34+00:00', 'url': 'https://arxiv.org/pdf/2601.14678v1', 'resource_uri': 'arxiv://2601.14678v1'}, {'id': '2601.17048v1', 'title': 'SiMiC: Context-Aware Silicon Microstructure Characterization Using Attention-Based Convolutional Neural Networks for Field-Emission Tip Analysis', 'authors': ['Jing Jie Tan', 'Rupert Schreiner', 'Matthias Hausladen', 'Ali Asgharzade', 'Simon Edler', 'Julian Bartsch', 'Michael Bachmann', 'Andreas Schels', 'Ban-Hoe Kwan', 'Danny Wee-Kiat Ng', 'Yan-Chai Hum'], 'abstract': 'Accurate characterization of silicon microstructures is essential for advancing microscale fabrication, quality control, and device performance. Traditional analysis using Scanning Electron Microscopy (SEM) often requires labor-intensive, manual evaluation of feature geometry, limiting throughput and reproducibility. In this study, we propose SiMiC: Context-Aware Silicon Microstructure Characterization Using Attention-Based Convolutional Neural Networks for Field-Emission Tip Analysis. By leveraging deep learning, our approach efficiently extracts morphological features-such as size, shape, and apex curvature-from SEM images, significantly reducing human intervention while improving measurement consistency. A specialized dataset of silicon-based field-emitter tips was developed, and a customized CNN architecture incorporating attention mechanisms was trained for multi-class microstructure classification and dimensional prediction. Comparative analysis with classical image processing techniques demonstrates that SiMiC achieves high accuracy while maintaining interpretability. The proposed framework establishes a foundation for data-driven microstructure analysis directly linked to field-emission performance, opening avenues for correlating emitter geometry with emission behavior and guiding the design of optimized cold-cathode and SEM electron sources. The related dataset and algorithm repository that could serve as a baseline in this area can be found at https://research.jingjietan.com/?q=SIMIC', 'categories': ['cs.CV', 'cs.AI'], 'published': '2026-01-21T04:36:42+00:00', 'url': 'https://arxiv.org/pdf/2601.17048v1', 'resource_uri': 'arxiv://2601.17048v1'}, {'id': '2601.11911v2', 'title': 'Reliable Deep Learning for Small-Scale Classifications: Experiments on Real-World Image Datasets from Bangladesh', 'authors': ['Alfe Suny', 'MD Sakib Ul Islam', 'Md. Imran Hossain'], 'abstract': 'Convolutional neural networks (CNNs) have achieved state-of-the-art performance in image recognition tasks but often involve complex architectures that may overfit on small datasets. In this study, we evaluate a compact CNN across five publicly available, real-world image datasets from Bangladesh, including urban encroachment, vehicle detection, road damage, and agricultural crops. The network demonstrates high classification accuracy, efficient convergence, and low computational overhead. Quantitative metrics and saliency analyses indicate that the model effectively captures discriminative features and generalizes robustly across diverse scenarios, highlighting the suitability of streamlined CNN architectures for small-class image classification tasks.', 'categories': ['cs.CV'], 'published': '2026-01-17T05:15:22+00:00', 'url': 'https://arxiv.org/pdf/2601.11911v2', 'resource_uri': 'arxiv://2601.11911v2'}, {'id': '2601.10084v1', 'title': 'Adaptive Label Error Detection: A Bayesian Approach to Mislabeled Data Detection', 'authors': ['Zan Chaudhry', 'Noam H. Rotenberg', 'Brian Caffo', 'Craig K. Jones', 'Haris I. Sair'], 'abstract': 'Machine learning classification systems are susceptible to poor performance when trained with incorrect ground truth labels, even when data is well-curated by expert annotators. As machine learning becomes more widespread, it is increasingly imperative to identify and correct mislabeling to develop more powerful models. In this work, we motivate and describe Adaptive Label Error Detection (ALED), a novel method of detecting mislabeling. ALED extracts an intermediate feature space from a deep convolutional neural network, denoises the features, models the reduced manifold of each class with a multidimensional Gaussian distribution, and performs a simple likelihood ratio test to identify mislabeled samples. We show that ALED has markedly increased sensitivity, without compromising precision, compared to established label error detection methods, on multiple medical imaging datasets. We demonstrate an example where fine-tuning a neural network on corrected data results in a 33.8% decrease in test set errors, providing strong benefits to end users. The ALED detector is deployed in the Python package statlab.', 'categories': ['cs.LG'], 'published': '2026-01-15T05:20:00+00:00', 'url': 'https://arxiv.org/pdf/2601.10084v1', 'resource_uri': 'arxiv://2601.10084v1'}, {'id': '2601.08959v1', 'title': 'Integrating APK Image and Text Data for Enhanced Threat Detection: A Multimodal Deep Learning Approach to Android Malware', 'authors': ['Md Mashrur Arifin', 'Maqsudur Rahman', 'Nasir U. Eisty'], 'abstract': 'As zero-day Android malware attacks grow more sophisticated, recent research highlights the effectiveness of using image-based representations of malware bytecode to detect previously unseen threats. However, existing studies often overlook how image type and resolution affect detection and ignore valuable textual data in Android Application Packages (APKs), such as permissions and metadata, limiting their ability to fully capture malicious behavior. The integration of multimodality, which combines image and text data, has gained momentum as a promising approach to address these limitations. This paper proposes a multimodal deep learning framework integrating APK images and textual features to enhance Android malware detection. We systematically evaluate various image types and resolutions across different Convolutional Neural Networks (CNN) architectures, including VGG, ResNet-152, MobileNet, DenseNet, EfficientNet-B4, and use LLaMA-2, a large language model, to extract and annotate textual features for improved analysis. The findings demonstrate that RGB images at higher resolutions (e.g., 256x256, 512x512) achieve superior classification performance, while the multimodal integration of image and text using the CLIP model reveals limited potential. Overall, this research highlights the importance of systematically evaluating image attributes and integrating multimodal data to develop effective malware detection for Android systems.', 'categories': ['cs.CR', 'cs.SE'], 'published': '2026-01-13T19:59:18+00:00', 'url': 'https://arxiv.org/pdf/2601.08959v1', 'resource_uri': 'arxiv://2601.08959v1'}, {'id': '2601.07316v1', 'title': 'BEAT-Net: Injecting Biomimetic Spatio-Temporal Priors for Interpretable ECG Classification', 'authors': ['Runze Ma', 'Caizhi Liao'], 'abstract': 'Although deep learning has advanced automated electrocardiogram (ECG) diagnosis, prevalent supervised methods typically treat recordings as undifferentiated one-dimensional (1D) signals or two-dimensional (2D) images. This formulation compels models to learn physiological structures implicitly, resulting in data inefficiency and opacity that diverge from medical reasoning. To address these limitations, we propose BEAT-Net, a Biomimetic ECG Analysis with Tokenization framework that reformulates the problem as a language modeling task. Utilizing a QRS tokenization strategy to transform continuous signals into biologically aligned heartbeat sequences, the architecture explicitly decomposes cardiac physiology through specialized encoders that extract local beat morphology while normalizing spatial lead perspectives and modeling temporal rhythm dependencies. Evaluations across three large-scale benchmarks demonstrate that BEAT-Net matches the diagnostic accuracy of dominant convolutional neural network (CNN) architectures while substantially improving robustness. The framework exhibits exceptional data efficiency, recovering fully supervised performance using only 30 to 35 percent of annotated data. Moreover, learned attention mechanisms provide inherent interpretability by spontaneously reproducing clinical heuristics, such as Lead II prioritization for rhythm analysis, without explicit supervision. These findings indicate that integrating biological priors offers a computationally efficient and interpretable alternative to data-intensive large-scale pre-training.', 'categories': ['cs.LG', 'cs.AI'], 'published': '2026-01-12T08:37:47+00:00', 'url': 'https://arxiv.org/pdf/2601.07316v1', 'resource_uri': 'arxiv://2601.07316v1'}, {'id': '2601.07035v1', 'title': 'Explainable Deep Radiogenomic Molecular Imaging for MGMT Methylation Prediction in Glioblastoma', 'authors': ['Hasan M Jamil'], 'abstract': 'Glioblastoma (GBM) is a highly aggressive primary brain tumor with limited therapeutic options and poor prognosis. The methylation status of the O6-methylguanine-DNA methyltransferase (MGMT) gene promoter is a critical molecular biomarker that influences patient response to temozolomide chemotherapy. Traditional methods for determining MGMT status rely on invasive biopsies and are limited by intratumoral heterogeneity and procedural risks. This study presents a radiogenomic molecular imaging analysis framework for the non-invasive prediction of MGMT promoter methylation using multi-parametric magnetic resonance imaging (mpMRI).\n  Our approach integrates radiomics, deep learning, and explainable artificial intelligence (XAI) to analyze MRI-derived imaging phenotypes and correlate them with molecular labels. Radiomic features are extracted from FLAIR, T1-weighted, T1-contrast-enhanced, and T2-weighted MRI sequences, while a 3D convolutional neural network learns deep representations from the same modalities. These complementary features are fused using both early fusion and attention-based strategies and classified to predict MGMT methylation status.\n  To enhance clinical interpretability, we apply XAI methods such as Grad-CAM and SHAP to visualize and explain model decisions. The proposed framework is trained on the RSNA-MICCAI Radiogenomic Classification dataset and externally validated on the BraTS 2021 dataset. This work advances the field of molecular imaging by demonstrating the potential of AI-driven radiogenomics for precision oncology, supporting non-invasive, accurate, and interpretable prediction of clinically actionable molecular biomarkers in GBM.', 'categories': ['cs.LG', 'cs.AI', 'cs.CV'], 'published': '2026-01-11T19:16:19+00:00', 'url': 'https://arxiv.org/pdf/2601.07035v1', 'resource_uri': 'arxiv://2601.07035v1'}, {'id': '2601.07856v1', 'title': 'Feature Entanglement-based Quantum Multimodal Fusion Neural Network', 'authors': ['Yu Wu', 'Qianli Zhou', 'Jie Geng', 'Xinyang Deng', 'Wen Jiang'], 'abstract': 'Multimodal learning aims to enhance perceptual and decision-making capabilities by integrating information from diverse sources. However, classical deep learning approaches face a critical trade-off between the high accuracy of black-box feature-level fusion and the interpretability of less outstanding decision-level fusion, alongside the challenges of parameter explosion and complexity. This paper discusses the accuracy-interpretablity-complexity dilemma under the quantum computation framework and propose a feature entanglement-based quantum multimodal fusion neural network. The model is composed of three core components: a classical feed-forward module for unimodal processing, an interpretable quantum fusion block, and a quantum convolutional neural network (QCNN) for deep feature extraction. By leveraging the strong expressive power of quantum, we have reduced the complexity of multimodal fusion and post-processing to linear, and the fusion process also possesses the interpretability of decision-level fusion. The simulation results demonstrate that our model achieves classification accuracy comparable to classical networks with dozens of times of parameters, exhibiting notable stability and performance across multimodal image datasets.', 'categories': ['quant-ph', 'cs.AI', 'cs.LG'], 'published': '2026-01-09T07:26:12+00:00', 'url': 'https://arxiv.org/pdf/2601.07856v1', 'resource_uri': 'arxiv://2601.07856v1'}]"
