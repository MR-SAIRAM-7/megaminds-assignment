total_results,papers
15,"[{'id': '2602.03614v1', 'title': 'Quantization-Aware Regularizers for Deep Neural Networks Compression', 'authors': ['Dario Malchiodi', 'Mattia Ferraretto', 'Marco Frasca'], 'abstract': 'Deep Neural Networks reached state-of-the-art performance across numerous domains, but this progress has come at the cost of increasingly large and over-parameterized models, posing serious challenges for deployment on resource-constrained devices. As a result, model compression has become essential, and -- among compression techniques -- weight quantization is largely used and particularly effective, yet it typically introduces a non-negligible accuracy drop. However, it is usually applied to already trained models, without influencing how the parameter space is explored during the learning phase. In contrast, we introduce per-layer regularization terms that drive weights to naturally form clusters during training, integrating quantization awareness directly into the optimization process. This reduces the accuracy loss typically associated with quantization methods while preserving their compression potential. Furthermore, in our framework quantization representatives become network parameters, marking, to the best of our knowledge, the first approach to embed quantization parameters directly into the backpropagation procedure. Experiments on CIFAR-10 with AlexNet and VGG16 models confirm the effectiveness of the proposed strategy.', 'categories': ['cs.LG'], 'published': '2026-02-03T15:07:43+00:00', 'url': 'https://arxiv.org/pdf/2602.03614v1', 'resource_uri': 'arxiv://2602.03614v1'}, {'id': '2602.03539v1', 'title': 'Optimal neural network approximation of smooth compositional functions on sets with low intrinsic dimension', 'authors': ['Thomas Nagler', 'Sophie Langer'], 'abstract': 'We study approximation and statistical learning properties of deep ReLU networks under structural assumptions that mitigate the curse of dimensionality. We prove minimax-optimal uniform approximation rates for $s$-Hölder smooth functions defined on sets with low Minkowski dimension using fully connected networks with flexible width and depth, improving existing results by logarithmic factors even in classical full-dimensional settings. A key technical ingredient is a new memorization result for deep ReLU networks that enables efficient point fitting with dense architectures. We further introduce a class of compositional models in which each component function is smooth and acts on a domain of low intrinsic dimension. This framework unifies two common assumptions in the statistical learning literature, structural constraints on the target function and low dimensionality of the covariates, within a single model. We show that deep networks can approximate such functions at rates determined by the most difficult function in the composition. As an application, we derive improved convergence rates for empirical risk minimization in nonparametric regression that adapt to smoothness, compositional structure, and intrinsic dimensionality.', 'categories': ['math.ST'], 'published': '2026-02-03T13:54:31+00:00', 'url': 'https://arxiv.org/pdf/2602.03539v1', 'resource_uri': 'arxiv://2602.03539v1'}, {'id': '2602.02988v1', 'title': 'NLI:Non-uniform Linear Interpolation Approximation of Nonlinear Operations for Efficient LLMs Inference', 'authors': ['Jiangyong Yu', 'Xiaomeng Han', 'Xing Hu', 'Chen Xu', 'Zhe Jiang', 'Dawei Yang'], 'abstract': ""Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of tasks, but their deployment is often constrained by substantial memory footprints and computational costs. While prior work has achieved significant progress in compressing and accelerating linear layers, nonlinear layers-such as SiLU, RMSNorm, and Softmax-still heavily depend on high-precision floating-point operations. In this paper, we propose a calibration-free, dynamic-programming-optimal, and hardware-friendly framework called Non-uniform Linear Interpolation (NLI). NLI is capable of efficiently approximating a variety of nonlinear functions, enabling seamless integration into LLMs and other deep neural networks with almost no loss in accuracy. NLI ingeniously recasts cutpoint selection as a dynamic-programming problem, achieving the globally minimal interpolation error in O(MxN2) time via Bellman's optimality principle. Based on the NLI algorithm, we also design and implement a plug-and-play universal nonlinear computation unit. Hardware experiments demonstrate that the NLI Engine achieves more than 4x improvement in computational efficiency compared to the state-of-the-art designs."", 'categories': ['cs.LG', 'cs.AI'], 'published': '2026-02-03T01:47:58+00:00', 'url': 'https://arxiv.org/pdf/2602.02988v1', 'resource_uri': 'arxiv://2602.02988v1'}, {'id': '2602.02439v1', 'title': 'Energy-Efficient Neuromorphic Computing for Edge AI: A Framework with Adaptive Spiking Neural Networks and Hardware-Aware Optimization', 'authors': ['Olaf Yunus Laitinen Imanov', 'Derya Umut Kulali', 'Taner Yilmaz', 'Duygu Erisken', 'Rana Irem Turhan'], 'abstract': 'Edge AI applications increasingly require ultra-low-power, low-latency inference. Neuromorphic computing based on event-driven spiking neural networks (SNNs) offers an attractive path, but practical deployment on resource-constrained devices is limited by training difficulty, hardware-mapping overheads, and sensitivity to temporal dynamics. We present NeuEdge, a framework that combines adaptive SNN models with hardware-aware optimization for edge deployment. NeuEdge uses a temporal coding scheme that blends rate and spike-timing patterns to reduce spike activity while preserving accuracy, and a hardware-aware training procedure that co-optimizes network structure and on-chip placement to improve utilization on neuromorphic processors. An adaptive threshold mechanism adjusts neuron excitability from input statistics, reducing energy consumption without degrading performance. Across standard vision and audio benchmarks, NeuEdge achieves 91-96% accuracy with up to 2.3 ms inference latency on edge hardware and an estimated 847 GOp/s/W energy efficiency. A case study on an autonomous-drone workload shows up to 312x energy savings relative to conventional deep neural networks while maintaining real-time operation.', 'categories': ['cs.NE', 'cs.ET', 'cs.LG'], 'published': '2026-02-02T18:34:48+00:00', 'url': 'https://arxiv.org/pdf/2602.02439v1', 'resource_uri': 'arxiv://2602.02439v1'}, {'id': '2602.02234v1', 'title': 'Enabling AI Deep Potentials for Ab Initio-quality Molecular Dynamics Simulations in GROMACS', 'authors': ['Andong Hu', 'Luca Pennati', 'Stefano Markidis', 'Ivy Peng'], 'abstract': 'State-of-the-art AI deep potentials provide ab initio-quality results, but at a fraction of the computational cost of first-principles quantum mechanical calculations, such as density functional theory. In this work, we bring AI deep potentials into GROMACS, a production-level Molecular Dynamics (MD) code, by integrating with DeePMD-kit that provides domain-specific deep learning (DL) models of interatomic potential energy and force fields. In particular, we enable AI deep potentials inference across multiple DP model families and DL backends by coupling GROMACS Neural Network Potentials with the C++/CUDA backend in DeePMD-kit. We evaluate two recent large-atom-model architectures, DPA2 that is based on the attention mechanism and DPA3 that is based on GNN, in GROMACS using four ab initio-quality protein-in-water benchmarks (1YRF, 1UBQ, 3LZM, 2PTC) on NVIDIA A100 and GH200 GPUs. Our results show that DPA2 delivers up to 4.23x and 3.18x higher throughput than DPA3 on A100 and GH200 GPUs, respectively. We also provide a characterization study to further contrast DPA2 and DPA3 in throughput, memory usage, and kernel-level execution on GPUs. Our findings identify kernel-launch overhead and domain-decomposed inference as the main optimization priorities for AI deep potentials in production MD simulations.', 'categories': ['cs.DC', 'physics.chem-ph', 'physics.comp-ph'], 'published': '2026-02-02T15:41:10+00:00', 'url': 'https://arxiv.org/pdf/2602.02234v1', 'resource_uri': 'arxiv://2602.02234v1'}, {'id': '2602.01996v1', 'title': 'Optimizing Tensor Train Decomposition in DNNs for RISC-V Architectures Using Design Space Exploration and Compiler Optimizations', 'authors': ['Theologos Anthimopoulos', 'Milad Kokhazadeh', 'Vasilios Kelefouras', 'Benjamin Himpel', 'Georgios Keramidas'], 'abstract': 'Deep neural networks (DNNs) have become indispensable in many real-life applications like natural language processing, and autonomous systems. However, deploying DNNs on resource-constrained devices, e.g., in RISC-V platforms, remains challenging due to the high computational and memory demands of fully connected (FC) layers, which dominate resource consumption. Low-rank factorization (LRF) offers an effective approach to compressing FC layers, but the vast design space of LRF solutions involves complex trade-offs among FLOPs, memory size, inference time, and accuracy, making the LRF process complex and time-consuming. This paper introduces an end-to-end LRF design space exploration methodology and a specialized design tool for optimizing FC layers on RISC-V processors. Using Tensor Train Decomposition (TTD) offered by TensorFlow T3F library, the proposed work prunes the LRF design space by excluding first, inefficient decomposition shapes and second, solutions with poor inference performance on RISC-V architectures. Compiler optimizations are then applied to enhance custom T3F layer performance, minimizing inference time and boosting computational efficiency. On average, our TT-decomposed layers run 3x faster than IREE and 8x faster than Pluto on the same compressed model. This work provides an efficient solution for deploying DNNs on edge and embedded devices powered by RISC-V architectures.', 'categories': ['cs.LG', 'cs.AI', 'cs.AR', 'cs.MS'], 'published': '2026-02-02T11:56:36+00:00', 'url': 'https://arxiv.org/pdf/2602.01996v1', 'resource_uri': 'arxiv://2602.01996v1'}, {'id': '2602.01981v1', 'title': 'Putting machine learning to the test in a quantum many-body system', 'authors': ['Yilun Gao', 'Alberto Rodríguez', 'Rudolf A. Römer'], 'abstract': 'Quantum many-body systems pose a formidable computational challenge due to the exponential growth of their Hilbert space. While machine learning (ML) has shown promise as an alternative paradigm, most applications remain at the proof-of-concept stage, focusing narrowly on energy estimation at the lower end of the spectrum. Here, we push ML beyond this frontier by extensively testing HubbardNet, a deep neural network architecture for the Bose-Hubbard model. Pushing improvements in the optimizer and learning rates, and introducing physics-informed output activations that can resolve extremely small wave-function amplitudes, we achieve ground-state energy errors reduced by orders of magnitude and wave-function fidelities exceeding 99%. We further assess physical relevance by analysing generalized inverse participation ratios and multifractal dimensions for ground and excited states in one and two dimensions, demonstrating that optimized ML models reproduce localization, delocalization, and multifractality trends across the spectrum. Crucially, these qualitative predictions remain robust across four decades of the interaction strength, e.g. spanning across superfluid, Mott-insulating, as well as quantum chaotic regimes. Together, these results suggest ML as a viable qualitative predictor of many-body structure, complementing the quantitative strengths of exact diagonalization and tensor-network methods.', 'categories': ['cond-mat.dis-nn', 'cond-mat.quant-gas'], 'published': '2026-02-02T11:36:39+00:00', 'url': 'https://arxiv.org/pdf/2602.01981v1', 'resource_uri': 'arxiv://2602.01981v1'}, {'id': '2602.01308v1', 'title': 'Dispelling the Curse of Singularities in Neural Network Optimizations', 'authors': ['Hengjie Cao', 'Mengyi Chen', 'Yifeng Yang', 'Fang Dong', 'Ruijun Huang', 'Anrui Chen', 'Jixian Zhou', 'Mingzhi Dong', 'Yujiang Wang', 'Dongsheng Li', 'Wenyi Fang', 'Yuanyi Lin', 'Fan Wu', 'Li Shang'], 'abstract': 'This work investigates the optimization instability of deep neural networks from a less-explored yet insightful perspective: the emergence and amplification of singularities in the parametric space. Our analysis reveals that parametric singularities inevitably grow with gradient updates and further intensify alignment with representations, leading to increased singularities in the representation space. We show that the gradient Frobenius norms are bounded by the top singular values of the weight matrices, and as training progresses, the mutually reinforcing growth of weight and representation singularities, termed the curse of singularities, relaxes these bounds, escalating the risk of sharp loss explosions. To counter this, we propose Parametric Singularity Smoothing (PSS), a lightweight, flexible, and effective method for smoothing the singular spectra of weight matrices. Extensive experiments across diverse datasets, architectures, and optimizers demonstrate that PSS mitigates instability, restores trainability even after failure, and improves both training efficiency and generalization.', 'categories': ['cs.LG', 'cs.AI'], 'published': '2026-02-01T16:09:06+00:00', 'url': 'https://arxiv.org/pdf/2602.01308v1', 'resource_uri': 'arxiv://2602.01308v1'}, {'id': '2602.01136v1', 'title': 'A Unified Matrix-Spectral Framework for Stability and Interpretability in Deep Learning', 'authors': ['Ronald Katende'], 'abstract': 'We develop a unified matrix-spectral framework for analyzing stability and interpretability in deep neural networks. Representing networks as data-dependent products of linear operators reveals spectral quantities governing sensitivity to input perturbations, label noise, and training dynamics.\n  We introduce a Global Matrix Stability Index that aggregates spectral information from Jacobians, parameter gradients, Neural Tangent Kernel operators, and loss Hessians into a single stability scale controlling forward sensitivity, attribution robustness, and optimization conditioning. We further show that spectral entropy refines classical operator-norm bounds by capturing typical, rather than purely worst-case, sensitivity.\n  These quantities yield computable diagnostics and stability-oriented regularization principles. Synthetic experiments and controlled studies on MNIST, CIFAR-10, and CIFAR-100 confirm that modest spectral regularization substantially improves attribution stability even when global spectral summaries change little.\n  The results establish a precise connection between spectral concentration and analytic stability, providing practical guidance for robustness-aware model design and training.', 'categories': ['cs.LG', 'math.DS', 'math.OC'], 'published': '2026-02-01T10:18:37+00:00', 'url': 'https://arxiv.org/pdf/2602.01136v1', 'resource_uri': 'arxiv://2602.01136v1'}, {'id': '2602.01131v1', 'title': 'Lyapunov Stability-Aware Stackelberg Game for Low-Altitude Economy: A Control-Oriented Pruning-Based DRL Approach', 'authors': ['Yue Zhong', 'Jiawen Kang', 'Yongju Tong', 'Hong-Ning Dai', 'Dong In Kim', 'Abbas Jamalipour', 'Shengli Xie'], 'abstract': 'With the rapid expansion of the low-altitude economy, Unmanned Aerial Vehicles (UAVs) serve as pivotal aerial base stations supporting diverse services from users, ranging from latency-sensitive critical missions to bandwidth-intensive data streaming. However, the efficacy of such heterogeneous networks is often compromised by the conflict between limited onboard resources and stringent stability requirements. Moving beyond traditional throughput-centric designs, we propose a Sensing-Communication-Computing-Control closed-loop framework that explicitly models the impact of communication latency on physical control stability. To guarantee mission reliability, we leverage the Lyapunov stability theory to derive an intrinsic mapping between the state evolution of the control system and communication constraints, transforming abstract stability requirements into quantifiable resource boundaries. Then, we formulate the resource allocation problem as a Stackelberg game, where UAVs (as leaders) dynamically price resources to balance load and ensure stability, while users (as followers) optimize requests based on service urgency. Furthermore, addressing the prohibitive computational overhead of standard Deep Reinforcement Learning (DRL) on energy-constrained edge platforms, we propose a novel and lightweight pruning-based Proximal Policy Optimization (PPO) algorithm. By integrating a dynamic structured pruning mechanism, the proposed algorithm significantly compresses the neural network scale during training, enabling the UAV to rapidly approximate the game equilibrium with minimal inference latency. Simulation results demonstrate that the proposed scheme effectively secures control loop stability while maximizing system utility in dynamic low-altitude environments.', 'categories': ['cs.AI'], 'published': '2026-02-01T10:01:07+00:00', 'url': 'https://arxiv.org/pdf/2602.01131v1', 'resource_uri': 'arxiv://2602.01131v1'}, {'id': '2602.01069v1', 'title': 'PDE-Constrained Optimization for Neural Image Segmentation with Physics Priors', 'authors': ['Seema K. Poudel', 'Sunny K. Khadka'], 'abstract': 'Segmentation of microscopy images constitutes an ill-posed inverse problem due to measurement noise, weak object boundaries, and limited labeled data. Although deep neural networks provide flexible nonparametric estimators, unconstrained empirical risk minimization often leads to unstable solutions and poor generalization. In this work, image segmentation is formulated as a PDE-constrained optimization problem that integrates physically motivated priors into deep learning models through variational regularization. The proposed framework minimizes a composite objective function consisting of a data fidelity term and penalty terms derived from reaction-diffusion equations and phase-field interface energies, all implemented as differentiable residual losses. Experiments are conducted on the LIVECell dataset, a high-quality, manually annotated collection of phase-contrast microscopy images. Training is performed on two cell types, while evaluation is carried out on a distinct, unseen cell type to assess generalization. A UNet architecture is used as the unconstrained baseline model. Experimental results demonstrate consistent improvements in segmentation accuracy and boundary fidelity compared to unconstrained deep learning baselines. Moreover, the PDE-regularized models exhibit enhanced stability and improved generalization in low-sample regimes, highlighting the advantages of incorporating structured priors. The proposed approach illustrates how PDE-constrained optimization can strengthen data-driven learning frameworks, providing a principled bridge between variational methods, statistical learning, and scientific machine learning.', 'categories': ['cs.CV', 'cs.LG'], 'published': '2026-02-01T07:28:14+00:00', 'url': 'https://arxiv.org/pdf/2602.01069v1', 'resource_uri': 'arxiv://2602.01069v1'}, {'id': '2602.00838v1', 'title': 'Exploration of Unary Arithmetic-Based Matrix Multiply Units for Low Precision DL Accelerators', 'authors': ['Prabhu Vellaisamy', 'Harideep Nair', 'Di Wu', 'Shawn Blanton', 'John Paul Shen'], 'abstract': ""General matrix multiplication (GEMM) is a fundamental operation in deep learning (DL). With DL moving increasingly toward low precision, recent works have proposed novel unary GEMM designs as an alternative to conventional binary GEMM hardware. A rigorous evaluation of recent unary and binary GEMM designs is needed to assess the potential of unary hardware for future DL compute. This paper focuses on unary GEMM designs for integer-based DL inference and performs a detailed evaluation of three latest unary design proposals, namely, uGEMM, tuGEMM and tubGEMM, by comparing them to a conventional binary GEMM. Rigorous post-synthesis evaluations beyond prior works are performed across varying bit-widths and matrix sizes to assess the designs' tradeoffs and determine optimal sweetspots. Further, we perform weight sparsity analysis across eight pretrained convolutional neural networks (CNNs) and the LLaMA2 large language model (LLM). In this work, we demonstrate how unary GEMM can be effectively used for energy-efficient compute in future edge AI accelerators."", 'categories': ['cs.AR', 'cs.AI'], 'published': '2026-01-31T18:10:55+00:00', 'url': 'https://arxiv.org/pdf/2602.00838v1', 'resource_uri': 'arxiv://2602.00838v1'}, {'id': '2602.00827v1', 'title': 'Over-Alignment vs Over-Fitting: The Role of Feature Learning Strength in Generalization', 'authors': ['Taesun Yeom', 'Taehyeok Ha', 'Jaeho Lee'], 'abstract': 'Feature learning strength (FLS), i.e., the inverse of the effective output scaling of a model, plays a critical role in shaping the optimization dynamics of neural nets. While its impact has been extensively studied under the asymptotic regimes -- both in training time and FLS -- existing theory offers limited insight into how FLS affects generalization in practical settings, such as when training is stopped upon reaching a target training risk. In this work, we investigate the impact of FLS on generalization in deep networks under such practical conditions. Through empirical studies, we first uncover the emergence of an $\\textit{optimal FLS}$ -- neither too small nor too large -- that yields substantial generalization gains. This finding runs counter to the prevailing intuition that stronger feature learning universally improves generalization. To explain this phenomenon, we develop a theoretical analysis of gradient flow dynamics in two-layer ReLU nets trained with logistic loss, where FLS is controlled via initialization scale. Our main theoretical result establishes the existence of an optimal FLS arising from a trade-off between two competing effects: An excessively large FLS induces an $\\textit{over-alignment}$ phenomenon that degrades generalization, while an overly small FLS leads to $\\textit{over-fitting}$.', 'categories': ['cs.LG', 'stat.ML'], 'published': '2026-01-31T17:43:02+00:00', 'url': 'https://arxiv.org/pdf/2602.00827v1', 'resource_uri': 'arxiv://2602.00827v1'}, {'id': '2602.00545v1', 'title': 'Depth, Not Data: An Analysis of Hessian Spectral Bifurcation', 'authors': ['Shenyang Deng', 'Boyao Liao', 'Zhuoli Ouyang', 'Tianyu Pang', 'Yaoqing Yang'], 'abstract': ""The eigenvalue distribution of the Hessian matrix plays a crucial role in understanding the optimization landscape of deep neural networks. Prior work has attributed the well-documented ``bulk-and-spike'' spectral structure, where a few dominant eigenvalues are separated from a bulk of smaller ones, to the imbalance in the data covariance matrix. In this work, we challenge this view by demonstrating that such spectral Bifurcation can arise purely from the network architecture, independent of data imbalance.\n  Specifically, we analyze a deep linear network setup and prove that, even when the data covariance is perfectly balanced, the Hessian still exhibits a Bifurcation eigenvalue structure: a dominant cluster and a bulk cluster. Crucially, we establish that the ratio between dominant and bulk eigenvalues scales linearly with the network depth. This reveals that the spectral gap is strongly affected by the network architecture rather than solely by data distribution. Our results suggest that both model architecture and data characteristics should be considered when designing optimization algorithms for deep networks."", 'categories': ['cs.LG'], 'published': '2026-01-31T06:17:00+00:00', 'url': 'https://arxiv.org/pdf/2602.00545v1', 'resource_uri': 'arxiv://2602.00545v1'}, {'id': '2602.00540v1', 'title': 'Surrogate Ensemble in Expensive Multi-Objective Optimization via Deep Q-Learning', 'authors': ['Yuxin Wu', 'Hongshu Guo', 'Ting Huang', 'Yue-Jiao Gong', 'Zeyuan Ma'], 'abstract': ""Surrogate-assisted Evolutionary Algorithms~(SAEAs) have shown promising robustness in solving expensive optimization problems. A key aspect that impacts SAEAs' effectiveness is surrogate model selection, which in existing works is predominantly decided by human developer. Such human-made design choice introduces strong bias into SAEAs and may hurt their expected performance on out-of-scope tasks. In this paper, we propose a reinforcement learning-assisted ensemble framework, termed as SEEMOO, which is capable of scheduling different surrogate models within a single optimization process, hence boosting the overall optimization performance in a cooperative paradigm. Specifically, we focus on expensive multi-objective optimization problems, where multiple objective functions shape a compositional landscape and hence challenge surrogate selection. SEEMOO comprises following core designs: 1) A pre-collected model pool that maintains different surrogate models; 2) An attention-based state-extractor supports universal optimization state representation of problems with varied objective numbers; 3) a deep Q-network serves as dynamic surrogate selector: Given the optimization state, it selects desired surrogate model for current-step evaluation. SEEMOO is trained to maximize the overall optimization performance under a training problem distribution. Extensive benchmark results demonstrate SEEMOO's surrogate ensemble paradigm boosts the optimization performance of single-surrogate baselines. Further ablation studies underscore the importance of SEEMOO's design components."", 'categories': ['cs.NE', 'cs.LG'], 'published': '2026-01-31T06:14:27+00:00', 'url': 'https://arxiv.org/pdf/2602.00540v1', 'resource_uri': 'arxiv://2602.00540v1'}]"
